{"goal": "\n    Create a blog post that relates to the datasources for a socail media AI company of around 300 words.\n    The company is called South River AI they are developing Athena a social media interaction application.\n    The application is designed to help users interact with social media in a more productive way. \n    It systhesizes posts from a variety of sources and presents them to the user in a way that is more condusive to productivity.\n    ", "list_data_source": [{"title": "Things are about to get a lot worse for Generative AI", "url": "https://garymarcus.substack.com/p/things-are-about-to-get-a-lot-worse", "text": "Things are about to get a lot worse for Generative AI\nMarcus on AI\nSubscribe\nSign in\nShare this post\nThings are about to get a lot worse for Generative AI\ngarymarcus.substack.com\nCopy link\nFacebook\nEmail\nNote\nOther\nThings are about to get a lot worse for Generative AI\nA full of spectrum of infringement\nGary Marcus\nDec 29, 2023\n119\nShare this post\nThings are about to get a lot worse for Generative AI\ngarymarcus.substack.com\nCopy link\nFacebook\nEmail\nNote\nOther\n73\nShare\nAt around the same time as news of the New York Times lawsuit vs OpenAI broke, Reid Southen, the film industry concept artist (Marvel, DC,\nMatrix Resurrections\n,\nHunger Games\n, etc.) I wrote about last week, and I started doing some experiments together.\nWe will publish a full report next week, but it is already clear that what we are finding poses serious challenges for generative AI.\nThe crux of the Times lawsuit is that OpenAI\u2019s chatbots are fully capable of reproducing text nearly verbatim:\n\u00a7\nThe thing is, it is not just text.  OpenAI\u2019s image software (which we accessed through Bing) is perfectly capable of verbatim and near-verbatim repetition of sources as well.\nDall-E already has one minor safeguard in place \u2013 proper names (and hence deliberate infringement attempts)\nreportedly\nsometimes get blocked \u2013 but those safeguards aren\u2019t fully reliable:\nAnd worse,  infringement can happen even the user isn\u2019t looking to infringe and doesn\u2019t mention any character or film by name:\nDall-E  can does the same kind of thing  with short prompts like this one,\nHere, just two words. The show\nSpongeBob SquarePants\nis never mentioned:\nNo mention of the film\nRoboCop\nVideo game characters\nAnd a whole universe of potential trademark infringements with this single two-word prompt:\n\u00a7\nA few minutes ago, a user on X, named Blanket_Man01 discovered essentially the same thing:\nBlanketman\u2019s\nMario experiment\n:\nJustine Moore of A16Z earlier today independently\nnoticed the same thing\n:\n\u00a7\nThe cat is out of the bag:\nGenerative AI systems like DALL-E and ChatGPT have been trained on copyrighted materials;\nOpenAI, despite its name, has not been transparent about what it has been trained on.\nGenerative AI systems are fully capable of producing materials that infringe on copyright.\nThey do not inform users when they do so.\nThey do not provide any information about the provenance of any of the images they produce.\nUsers may not know when they produce any given image whether they are infringing.\n\u00a7\nMy guess is that none of this can easily be fixed.\nSystems like DALL-E and ChatGPT are essentially black boxes. GenAI systems don\u2019t give attribution to source materials because at least as constituted now, they\ncan\u2019t\n. (Some companies are researching how to do this sort of thing, but I am aware of no compelling solution thus far.)\nUnless and until somebody can invent a new architecture that can\nreliably\ntrack provenance of generative text and/or generative images, infringement \u2013 often not at the request of the user \u2014 will continue.\nA good system should give the user a manifest of sources; current systems don\u2019t.\nIn all likelihood, the New York Times lawsuit is just  the first of many. On a multiple choice X poll today I asked people whether they thought the case would settle (most did) and\nwhat the likely value of such a settlement might b\ne. Most answers were $100 million or more, 20% expected the settlement to be a billion dollars. When you multiply figures like these by the number of film studios, video game companies, other newspapers etc, you are soon talking real money.\nAnd OpenAI faces further risks.\nAnd because the stuff we reported on above was all done through Bing using Dall-E, Microsoft is on the hook, too.\nMore about all this on January 3, at\nIEEE Spectrum\n.\nIf you care about artists, please consider sharing this post\nShare\nGary Marcus\nis a scientist and best-selling author who spoke before US Senate in May on AI Oversight. He was Founder and CEO of Geometric Intelligence, a machine learning company he sold to Uber.\nReid Southen\n, his collaborator on this work, is film industry concept artist who was worked with many major studios (Marvel, DC,  ) and on many major films (\nMatrix Resurrections\n,\nHunger Games\n, etc)\nSubscribe\n119\nShare this post\nThings are about to get a lot worse for Generative AI\ngarymarcus.substack.com\nCopy link\nFacebook\nEmail\nNote\nOther\n73\nShare\n73 Comments\nShare this discussion\nThings are about to get a lot worse for Generative AI\ngarymarcus.substack.com\nCopy link\nFacebook\nEmail\nNote\nOther\nRoumen Popov\n16 hrs ago\nLiked by Gary Marcus\nDuring training the models minimize the difference between their output and the training data, so if the model is well trained it would output stuff very close to the training data under the same conditions that were used during the training, so it could be argued that a very close copy of the training data exists encoded in the model's weights and can be obtained with the right prompting. Isn't that grounds for copyright infringement? It's like a jpegged copy of an image is still a copy even though the image is encoded and to get it one needs a jpeg decoder.\nExpand full comment\nReply\nShare\n1 reply\nPaul Topping\n17 hrs ago\n\u00b7\nedited 17 hrs ago\nLiked by Gary Marcus\nI find the AI industry's answers to accusations of infringement ludicrous. They are using huge amounts of IP they don't own as source code for a software application and making money off it. Calling it \"training\" and \"learning\" changes nothing. The fact their process converts the content into billions of parameters changes nothing.\nStatistical analysis of content is fair use. It is equivalent to reading. But as soon as LLMs start producing words and expressions of ideas that are solely based on this massive amount of ripped off data, they are competing with the owners of the IP. This is infringement, plain and simple.\nAs the US Copyright Office says: \"What is copyright infringement? As a general matter, copyright infringement occurs when a copyrighted work is reproduced, distributed, performed, publicly displayed, or made into a derivative work without the permission of the copyright owner.\"\nThe fact that they infringe many copyrights shouldn't let them off the hook either. Rip off one copyright holder, you are infringing. Rip off thousands or millions of them, then you're ok. I don't think so!\nExpand full comment\nReply\nShare\n71 more comments...\nTop\nNew\nCommunity\nNo posts\nReady for more?\nSubscribe\n\u00a9 2023 Gary Marcus\nPrivacy\n\u2219\nTerms\n\u2219\nCollection notice\nStart Writing\nGet the app\nSubstack\nis the home for great writing\nOur use of cookies\nWe use necessary cookies to make our site work. We also set performance and functionality cookies that help us make\n  improvements by measuring traffic on our site. For more detailed information about the cookies we use, please see our\nprivacy policy\n.\n\u2716\nThis site requires JavaScript to run correctly. Please\nturn on JavaScript\nor unblock scripts"}, {"title": "How is AI impacting science?", "url": "https://michaelnotebook.com/mc2023/index.html", "text": "How is AI impacting science?\nai\nmetascience\nfom\ntalk\nHow is AI impacting science?\nMichael Nielsen\nAstera Institute\nMay 14, 2023\nThe biggest\n      success of AI in science so far is the AlphaFold 2 system. This is a\n      deep learning system which has made large strides on a fundamental\n      scientific problem: how to predict the 3-dimensional structure of a\n      protein from the sequence of amino acids making up that protein.  This\n      breakthrough has helped set off an ongoing deep learning revolution in\n      molecular biology.  While obviously of interest to molecular\n      biologists, I believe this is of much broader interest for science as\n      a whole, as a concrete prototype for how artificial intelligence may\n      impact discovery. In this short survey talk I briefly discuss\n      questions including: how can such systems be validated?  Can they be\n      used to identify general principles that human scientists can learn\n      from? And what should we expect a good theory or explanation to\n      provide anyway?  The focus of the talk is extant results and the near\n      term, not the longer-term future.  However, I hope it may help in\n      grounding an understanding of that longer-term future, and of the\n      benefits and risks of AI systems.\nThe text for a talk given at the\nMetascience 2023 Conference\nin Washington, D.C., May 2023.\nIn 2020 the deep learning system AlphaFold 2\n1\nsurprised biologists when it was shown to routinely make correct near atomic-precision predictions for protein structure. That is, using just the linear sequence of amino acids making up a protein, AlphaFold was able to predict the positions of the atoms in the protein. These results were not cherrypicked, but rather emerged from an adversarial, blind competition with over a hundred other modeling groups. In a few cases, AlphaFold\n2\nhas even exceeded experimental accuracy, causing existing experimental results to be re-evaluated and improved.\nWhile AlphaFold is impressive, it's also far from complete: it's really a bridge to a new era, opening up many scientific and metascientific questions. These include: what we expect a good theory or explanation to provide; what it means to validate that understanding; and what we humans can learn from these systems. Most of all: whether and how AI systems may impact the progress of science as a whole, as a systemic intervention. In my talk, I'll treat AlphaFold as a concrete prototype for how AI may be used across science. For these reasons, it's valuable for metascientists to engage with AlphaFold and successor systems, even if you have no prior interest in proteins or even in biology.\nThe talk is a survey. I am not a molecular biologist, so my apologies for any errors on that front. That said, it's been enjoyable and often inspiring to learn about proteins. Let me take a few minutes to remind you of some background for those of you (like me) who aren't biologists. Molecular biology feels a bit like wandering into an immense workshop full of wonderful and varied machines. Large databases like UniProt contain the amino acid sequences for hundreds of millions of proteins. You have, for example, kinesin proteins, which transport large molecules around the interior of the cell. You have haemoglobin, which carries oxygen in your blood, and helps power your metabolism. You have green fluorescent protein, which emits green light when exposed to ultraviolet light, and which can be used to tag and track other biomolecules. All these and many more molecular machines, created and sorted by the demanding sieve of evolution by natural selection. Every individual machine could be the subject of a lifetime's study. There are, for instance, thousands of papers about the kinesin superfamily, and yet we're really just beginning to understand it. But while this wealth of biological machines is astonishing, we don't\na priori\nknow what those machines do, or how they do it. We have no instruction manual, and we're trying to figure it out.\nIn fact, for the vast majority of the hundreds of millions of proteins known\n3\n, all we can easily directly determine is the basic blueprint: using genome sequencing we can find the linear sequence of amino acids that form the protein, at a cost of no more than cents. But proteins are tiny 3-dimensional structures, typically nanometers across, making it extremely difficult to directly image them. For a single protein it will routinely take\nmonths\nof work to experimentally determine the corresponding 3-dimensional structure, usually using x-ray crystallography or cryo-electron microscopy or NMR.\nThis discrepancy really matters. It matters because understanding the shape is crucial for understanding questions like:\nWhat antigens can an antibody protein bind to, as part of an immune response?\nWhat can the protein carry around? [E.g., the way haemoglobin carries oxygen]\nHow do proteins form larger complexes? [E.g., the ribosome]\nUnderstanding the shape of a protein doesn't tell you everything about its function. But it is fundamental to understanding what a protein can do, and how it does it.\nIdeally, we'd be able to determine the shape from the amino acid sequence alone. There are reasons chemists and biologists expect this to often be possible\n4\n, and in the 1970s, scientists began doing physics simulations to attempt to determine the shape a protein will fold into. Many techniques have been adopted since (kinetics, thermodynamics, evolutionary, \u2026.) For a long time these made only very slow progress. And even for that progress, one must wonder if modelers are cherrypicking, even with the best will in the world. There were, for instance, claims in the 1990s from Johns Hopkins to have largely \"solved\" the protein structure prediction problem\n5\n.\nTo assess progress in a fair but demanding way, in 1994 a competition named\nCASP\n(Critical Assessment of protein Structure Prediction) was begun. Running every two years, CASP asks modelers to do\nblind predictions\nof protein structure. That is, they're asked to predict structures for proteins whose amino acid sequence is known, but where biologists are still working on experimentally determining the three-dimensional structure, and expect to know it shortly after the competition is over, so it can be used to score predictions. The score for any given model is (very roughly) what percentage of amino acids positions are predicted correctly, according to some demanding threshold\n6\n. As you can see, through the 2010s the winner would typically score in the range 30 to 50\n7\n:\nThat is, the winner would (roughly) place somewhat than half of amino acids near perfectly. Then, in 2018 and 2020, DeepMind entered with AlphaFold and AlphaFold 2:\nI'll focus on CASP 14 in 2020, when AlphaFold 2 achieved of score of 87, so roughly 87 percent of amino acids within a demanding threshold. This was AlphaFold's score in the most demanding category, the free modeling category, where contestants are asked to predict the structure of proteins for which few similar proteins were previously known. Across all categories, AlphaFold predicted the position of the alpha-carbon atoms in the backbone with a root mean square distance of 0.96 Angstroms\n8\n. The next-best technique was roughly one third as accurate, with a root mean square distance of 2.8 Angstroms\n9\n:\nFor comparison, the van der Waals diameter of a carbon atom is roughly 1.4 Angstroms. AlphaFold's accuracy is often comparable to experimental accuracy. The co-founder of CASP, John Moult of the University of Maryland, said\n10\n:\nWe have been stuck on this one problem \u2013 how do proteins fold up \u2013 for nearly 50 years. To see DeepMind produce a solution for this, having worked personally on this problem for so long and after so many stops and starts, wondering if we\u2019d ever get there, is a very special moment.\nThat is a very strong statement, and it's worth digging into in what sense AlphaFold solves the protein structure prediction problem, and what remains to be understood. However, even AlphaFold's competitors were laudatory. Here's Mohammed AlQuraishi of Columbia University\n11\n:\nDoes this constitute a solution of the static protein structure prediction problem? I think so but there are all these wrinkles. Honest, thoughtful people can disagree here and it comes down to one\u2019s definition of what the word \u201csolution\u201d really means\u2026 the bulk of the scientific problem is solved; what\u2019s left now is execution.\nThat was two-and-a-half years ago. I think a reasonable broad view today is: AlphaFold is a huge leap, but much remains to be done even on the basic problem, and many enormous new problems can now be attacked.\nAlphaFold's high-level architecture would take several hours to describe. I want to emphasize just a few points today:\nIt's a deep neural network, meaning a hierarchical model, with 93 million parameters learned through training. An amino acid sequence is input, and a 3-dimensional structure is output, together with some error estimates quantifying how confident AlphaFold is in the placement of each amino acid. The basic training data is the protein data bank (PDB), humanity's repository of the protein structures experimentally determined since the 1970s. At training time, that was 170,000 protein structures (though a small fraction were omitted for technical reasons). The parameters in the AlphaFold network are adjusted using gradient descent to ensure the network outputs the correct structure, given the input.\nThat's a (very!) broad picture of how the network learned to predict structures. Many more ideas were important. One clever and important (albeit existing) idea was a way to learn from the hundreds of millions of known sequences for proteins. The idea is to find many other proteins whose amino acid sequences are similar to the input protein, meaning they're likely to be evolutionarily related. Maybe, for instance, it's essentially the same protein, but in some other species. AlphaFold tries to find many such similar proteins, and to learn from them. To get an intuition for how it might do that, suppose by looking at many similar proteins AlphaFold finds that there is some particular pair of amino acids which are a long way apart in the linear chain, but where changes in one amino acid seem to be correlated to changes in the other. If you saw that you might suspect these amino acids are likely to be close together in space, and are co-evolving together to preserve the shape of the protein. In fact, this often does happen, and it provides a way AlphaFold can learn both from\nknown structural information\n(in the PDB), and from\nknown evolutionary information\n(in protein sequence databases). A nice way of putting it, in\na talk from John Jumper\n, lead author on the AlphaFold paper, is that the physics informs AlphaFold's understanding of the evolutionary history, and the evolutionary history informs AlphaFold's understanding of the physics.\nGeneralization and reliability\nYou might wonder: is AlphaFold just memorizing its training data, or can it generalize? CASP provides a basic validation: the competition structures were not in the PDB at the time AlphaFold predicted them. Furthermore, CASP is in some sense a \"natural\" sample: the structural biology community prefers to solve structure which are biologically important\n12\n. So it suggests some ability to generalize to proteins of interest to biologists at large.\nWhat if we use deep learning to study proteins which don't occur in nature, maybe as the result of mutations, or as part of protein design? There's a huge amount of work going on, and frankly it's an exciting mess right now, all over the place \u2013 any reasonable overview would cover dozens if not hundreds of papers. There are papers saying AlphaFold works\nbadly\nfor such-and-such a type of mutation, it works\nwell\nfor such-and-such a kind of mutation, all sorts of deep learning fixes for protein design, and so on. My takeaway: it's going to keep biologists busy for years figuring out the shortcomings, and improving the systems to address those shortcomings.\nSome interesting tests of deep learning's ability to generalize were done by OpenFold\n13\n, an open source near-clone of AlphaFold. For instance, they retrained OpenFold with most broad classes of topology simply removed from the training set. To do this, they used a standard classification scheme, the CATH classification of protein topology. They removed 95% (!!!) of the topologies from the training set, and then retrained OpenFold from scratch. Even with the great majority of topologies removed, performance was similar to AlphaFold 1\n14\n: just a couple of years prior to AlphaFold 2 it would have been state-of-the-art.\nIn another variation on this idea, they retrained OpenFold starting from much smaller subsamples of the protein data bank. Instead of using 170,000 structures, they retrained with training sets as small as 1,000 structures, chosen at random. Even with such a tiny training set, they achieved a performance comparable to (in fact, slightly better than) AlphaFold 1. And with just 10,000 structures as training data they achieved a performance comparable to the full OpenFold.\nAs a practical matter, the question of AlphaFold generalization matters in part because DeepMind has released AlphaFold DB, a database of 215 million protein structures, including the (almost-complete) proteomes of 48 species, including humans, mice, and fruit flies. These were obtained by taking genetic sequences in UniProt, and then using AlphaFold to predict the structure. You can view the whole process as:\nIt's an astonishing act of generalization. If we had a perfectly reliable model, it would extend our understanding of protein structures by roughly three orders of magnitude. It's worth emphasizing that no additional experiments were done by AlphaFold; no additional data were taken. And yet by \"just thinking\" it was possible to obtain a very large number of predictions that people expect to be very good. I've heard from several biologists variations on the sentiment: \"No-one would take an AlphaFold prediction as true on its own; but it\u2019s an extremely helpful starting point, and can save you months of work\".\nAs the models get better still, I expect the line between model and experiment to become blurry. That may sound strange, but in fact traditional \"experimentally-determined structures\" actually require immensely-complicated theories to go from data to structure. If you believe AlphaFold (or a successor) offered a stronger theory, you might end up believing the predictions from the deep learning system more than you believe (today's) \"experimental results\". There are early hints of this beginning to happen. In the CASP assessment, AlphaFold performed poorly on several structures which had been determined using NMR spectroscopy. A 2022 paper\n15\nexamined \"904 human proteins with both Alpha-Fold and NMR structures\" and concluded that \"Alpha-Fold predictions are usually more accurate than NMR structures\". One of the authors, Mike Williamson, actually helped pioneer the use of NMR for structural biology.\nTo make the same point in a simpler setting: how we interpret the images from a telescope depends upon our theory of optics; if we were to change or improve our theory of optics, our understanding of the so-called \"raw data\" from the telescope would change. Indeed, something related has really happened in our understanding of the bending of light by gravitation. In that case, we've changed our understanding of the way light travels through space, and that has affected the way we interpret experimental data, particularly in understanding phenomena like gravitational lensing of distant galaxies.\nIn a similar way, \"experimental\" protein structure determination depends strongly on theory. You can get a gist for this in x-ray crystallography, which requires many difficult steps: purification of protein sample; crystallization (!) of the proteins; x-ray diffraction to obtain two-dimensional images; procedures to invert and solve for the three-dimensional structure; criteria for when the inversion is good enough. A lot of theory is involved! Indeed, the inversion procedure typically involves starting with a good \"guess\", a candidate search structure. Often people use related proteins, but sometimes they can't find a good search structure, and this can prevent a solution being found. AlphaFold has been used to find good search structures to help invert data for particularly challenging structures. So there is already a blurry line between theory and experiment. I expect figuring out how to validate AI \"solutions\" to problems will be a major topic of scientific and metascientific interest in the years to come.\nIs a simple set of principles for protein structure possible? Might AI help us discover it?\nAny model with 93 million learned parameters is complicated. It's not a \"theory\" or \"explanation\" in the conventional sense. You might wonder: can AlphaFold (or a successor) be used to help discover such a theory, even if only partial? Might, for instance, a simple set of principles for protein structure prediction be possible? And what, exactly, is AlphaFold 2 learning? To avoid disappointment, let me say: we don't yet know the answers to such questions. But pursuing them is a useful intuition pump for thinking about the role of AI in science.\nOne approach to finding such principles is \"behaviorist artificial psychology\": inferring high-level principles by observing the behavior of the system. Of course, AlphaFold's detailed predictions have already been used widely by biologists, for things like discovering new binding sites on proteins. But while this is very useful, it isn't the same as inferring novel high-level principles about protein structure. However, there are other important deep learning systems in which interesting new high-level principles have been found by observing behavior.\nFor instance, by observing the AlphaZero chess system\n16\nbehaviors have been inferred which violated conventional chess grandmaster wisdom. These behaviors were announced in December 2018\n17\nand January 2019\n18\n. A recent paper\n19\nexamined these behaviors, and tried to determine how (and whether) human players had changed in response to the system. They found few changes in the top ten players in 2019, with one exception: the world's top player then and now, Magnus Carlsen. They identify multiple ways in which Carlsen changed his play significantly in 2019, plausibly influenced by AlphaZero.\nAmong the changes: Carlsen advanced his h pawn early in the game\n20\nmuch more frequently (a 333% increase with white, a 175% increase with black). He changed his opening strategies for both white and black. Indeed, his two most common 2019 opening strategies with white (variations of the Queen's Gambit declined and of the Grunfeld Defense) were openings he didn't play at all in 2018. And with black he played the Sicilian less than 10% of the time in 2018, but 45% of the time in 2019. They also observed an increase in Carlsen's willingness to sacrifice material. All these changes were consistent with learning from AlphaZero. Carlsen lost no classical games in 2019, and was the only top grandmaster to considerably improve his Elo rating, despite already being the top-rated player: he increased by 37 rating points. By contrast, 6 of the top 10 players actually lost points, and none of the other 3 gained more than 6 points. Of course, we do not know how much of this was due to Carlsen learning from AlphaZero, but the paper makes a plausible case that Carlsen learned much from AlphaZero.\nSuch behavioral observation is interesting but frustrating: it doesn't tell us\nwhy\nthese behaviors occur. We may know that AlphaZero likes to advance the h pawn early, but what we'd really like is to understand why. Can we instead look inside the neural networks, and understand how they do what they do? As far as I know this kind of investigation has only been done casually for AlphaFold. But for simpler neural nets people are discovering interesting structure. Last year, for example, Neel Nanda and Tom Lieberum\n21\ntrained a single-layer transformer neural network to add two integers modulo 113. At first, the network simply memorized all the examples in the training set. It could add those well, but (unsurprisingly) gave terrible performance elsewhere. But as they trained the network for much longer, it transitioned to performing vastly better on held-out examples. Somehow, with no additional training data, it was learning to add.\nNanda and Lieberum spent weeks looking inside the network to understand what had changed. By reverse engineering the network they discovered it had learned to add in a remarkable way. The rough gist is: given numbers\nx\nand\ny\nthe network computed a wave\ne^{2\\pi i\n      kx/113}\n, then did a phase shift to\ne^{2\\pi i k x/ 113} \\times\n      e^{2\\pi i ky/113}\n, and finally tried to find a canceling phase shift\nz\nwhich would result in the wave\ne^{2 \\pi i k(x+y-z)/113}\nhaving the flattest possible \"tone\". It's a radio frequency engineer or group representation theorist's way of doing addition. This came as quite a surprise to Nanda and Lieberum. As Nanda said:\nTo emphasize, this algorithm was purely learned by gradient descent! I did not predict or understand this algorithm in advance and did nothing to encourage the model to learn this way of doing modular addition. I only discovered it by reverse engineering the weights.\nThis sequence of events greatly puzzled me when first I heard it: why did the network switch to this more general algorithm? After all, it initially memorized the training data, and provided excellent performance: why change? The answer is that during training the neural network pays a cost for more complex models: the loss function is chosen so gradient descent prefers lower-weight models. And the wave algorithm is actually lower-weight. It's a kind of mechanical implementation of Occam's razor, preferring an approach which is simpler and, in this case, more general. Indeed, by varying the loss function you can potentially impose Occam's razor in many different ways. It's intriguing to wonder: will one day we be able to do similar things for systems like AlphaFold, perhaps discovering new basic principles of protein structure?\nConcluding thought\nI hope you've enjoyed this brief look at AlphaFold and reflections on the use of AI in science. Of course, there's far more to say. But I believe it's clear there are many fundamental scientific and metascientific questions posed by AI systems. Most of all: as they get better at different types of cognitive operation, and as they become more able to sense and actuate in the environment, how will they impact science as a whole? Will AI systems eventually systematically change the entire practice of science? Will they perhaps greatly speed up scientific discovery? And if so, what risks and benefits does that carry?\nAfterword: what about messier problems, with less good data to learn from?\nAt the end of my talk many interesting questions were asked by audience members. One striking question came from Evan Miyazono. It was, roughly: \"As a target for AI, protein structure prediction benefits from having a lot of very clean prior data which the system can learn from. How useful do you think AI will be for problems that don't have lots of clean prior data to learn from?\"\nIt's a great question. A year or so ago I would have said that yes, it seems challenging to apply AI when you're working with messier problems with less clear metrics of success. But I've changed my mind over the year, as I've better understood foundation models, transfer learning, and zero-shot learning.\nTo explain why I've changed my mind, let me give two pieces of context. First, some informal gossip: many people have observed that ChatGPT is much better at coding than GitHub Copilot. I don't know the reason for sure. But a common speculation I've heard from informed people is that training on lots of text (as well as code) substantially improves the model's ability to code. Somehow, it seems the regularities in the text are improving ChatGPT's ability to understand code as well.\nThe second piece of context: a similar idea is used with AlphaFold. To explain this I need to explain a piece of AlphaFold's training that I didn't discuss in the body of the talk: it actually implicitly includes a large language model, treating the sequence of amino acids as a kind of \"text\" to be filled in. To understand this, recall that given a sequence of amino acids, AlphaFold looks up similar sequences in existing genetic databases. And during training, just like a language model, AlphaFold masks out (or mutates) some of the individual amino acids in those sequences, and attempts to predict the missing values. Doing this forces the network to learn more about the structure of the evolutionary information; it then takes advantage of that forced understanding to do better at protein structure prediction. In this way, AlphaFold benefits from transfer learning from a related domain (genetics) where far more information is available.\nReturning to the broader context of science, I expect large multi-modal foundation models will: (a) gradually begin to outperform special-purpose systems, just as large language models often outperform more specialized natural language algorithms; and (b) they will exhibit zero-shot or few-shot learning. These large multi-modal models will be trained not just on text, but also on images and actions and code and genetic data and geographic data and all sorts of sensor and actuator data. And, just as in the language models, they will use what they understand in one domain to help inform their work in adjacent, often messier domains, which the model may perhaps have limited information about. Indeed, there are already hints of protein language models moving in this direction.\nOf course, all this is merely a story and an intuition. It hasn't happened yet, except in very nascent ways, and I may be quite wrong! I certainly expect new ideas will be needed to make this work well. But hopefully that sums up how my intuition has changed over the last year: I think AI models will, in the next few years, be surprisingly good at addressing messy problems with only a little prior data. And they'll do it using multi-model foundation models, transfer learning, and zero- and few-shot learning.\nAcknowledgments\nThis talk was supported by the Astera Institute. Thanks to Alexander Berger, David Chapman, Evan Miyazono, Neel Nanda, and Pranav Shah for comments that helped improve this talk. And thanks to my many correspondents on Twitter, who have helped me deepen my understanding of AI and of molecular biology.\nCitation information\nFor attribution in academic contexts, please cite this work as:\nMichael Nielsen, \"How AI is impacting science\",\nhttps://michaelnotebook.com/mc2023/index.html\n, San Francisco (2023).\nFootnotes\nJohn Jumper, Richard Evans, Alexander Pritzel\net al\n,\nHighly accurate protein structure prediction with AlphaFold\n,\nNature\n(2021). See also\nthis colab\n, which provides an executable version of AlphaFold from Google DeepMind, the company behind AlphaFold. Note that other sources provide faster implementations, with comparable performance. See, for instance this\nColabFold colab\n. And, as we shall see later, other groups have developed open source implementations of the code training the system, not just for making structure predictions.\n\u21a9\ufe0e\nI'll use \"AlphaFold\" to refer to AlphaFold 2. Of course, it is a followup to an earlier AlphaFold system for protein structure prediction. However, we won't discuss that system in any depth, and so I will usually omit the number. Note that AlphaFold 1 and AlphaFold 2 had very different architectures, apart from both being deep learning systems for protein structure prediction.\n\u21a9\ufe0e\nIndeed, billions may be found (though of less clear provenance!) in metagenomics databases.\n\u21a9\ufe0e\nThis is a complicated subject, sometimes referred to as Anfinsen's hypothesis or Anfinsen's dogma. It is not true all the time \u2013 there are certainly circumstances under which the same amino acid sequence can fold in different ways, or has no stable structure. And yet it has served as an extremely useful basis for beginning to understand proteins and their function. In this talk I'll mostly assume it's true.\n\u21a9\ufe0e\nSee, for instance, this\nremarkable press release\nfrom Johns Hopkins, about the work of George Rose and Rajgopal Srinivasan \"solving\" the problem of predicting the backbone structure for globular proteins.\n\u21a9\ufe0e\nMore precisely, the global distance test (GDT) used in CASP is an average of four different percentages, measuring the fraction of predictions for the backbone alpha carbon atoms which fall within 1, 2, 4, and 8 Angstroms of the experimental structures. For comparison, I am told that good experimental structures are often thought to be accurate to about 1-2 Angstroms; better has been achieved, but worse is not uncommon, depending on the technique.\n\u21a9\ufe0e\nThis graph and the one that follow are adapted from\nAlphaFold: a solution to a 50-year-old grand challenge in biology\n(2021).\n\u21a9\ufe0e\nThis was for 95% coverage of the alpha carbons. For 100% coverage, it was 1.5 Angstroms.\n\u21a9\ufe0e\nFigure from: John Jumper, Richard Evans, Alexander Pritzel\net al\n,\nHighly accurate protein structure prediction with AlphaFold\n,\nNature\n(2021).\n\u21a9\ufe0e\nQuoted in:\nAlphaFold: a solution to a 50-year-old grand challenge in biology\n(2021).\n\u21a9\ufe0e\nMohammed AlQuraishi,\nAlphaFold2 @ CASP14: \u201cIt feels like one\u2019s child has left home.\u201d\n(2020).\n\u21a9\ufe0e\nOf course, other concerns like the experimental tractability of a structure also play a role.\n\u21a9\ufe0e\nGustaf Ahdritz, Nazim Bouatta, Sachin Kadyan\net al\n,\nOpenFold: Retraining AlphaFold2 yields new insights into its learning mechanisms and capacity for generalization\n(2022).\n\u21a9\ufe0e\nThey use the local distance difference test (lDDT-C\n\\alpha\n) as their metric, rather than the global distance test emphasized in AlphaFold's performance on CASP. I doubt this makes much difference, but haven't checked; I must admit, I wonder if there's some important point I'm missing here.\n\u21a9\ufe0e\nNicholas J. Fowler and Mike P. Williamson,\nThe accuracy of protein structures in solution determined by AlphaFold and NMR\n,\nStructure\n(2022).\n\u21a9\ufe0e\nDavid Silver, Thomas Hubert, Julian Schrittwieser\net al\n,\nMastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm\n(2017).\n\u21a9\ufe0e\nDavid Silver, Thomas Hubert, Julian Schrittwieser\net al\n, \"A General Reinforcement Learning Algorithm That Masters Chess, Shogi, and Go Through Self-Play\",\nScience\n(2018).\n\u21a9\ufe0e\nNatasha Regan and Matthew Sadler, \"Game Changer: AlphaZero\u2019s Groundbreaking Chess Strategies and the Promise of AI\" (2019).\n\u21a9\ufe0e\nJulio Gonz\u00e1lez-D\u00edaz and Ignacio Palacios-Huerta,\nAlphaZero Ideas\n(2022).\n\u21a9\ufe0e\nAll of this is for classical games.\n\u21a9\ufe0e\nThe original work was done by Neel Nanda and Tom Lieberum, and presented in\nA Mechanistic Interpretability Analysis of Grokking\n(2022). A fuller writeup may be found in: Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt,\nProgress measures for grokking via mechanistic interpretability\n(2023).\n\u21a9\ufe0e"}, {"title": "Pushing ChatGPT's Structured Data Support to Its Limits", "url": "https://minimaxir.com/2023/12/chatgpt-structured-data/", "text": "Pushing ChatGPT's Structured Data Support To Its Limits | Max Woolf's Blog\nMax Woolf's Blog\nPosts\nSearch\nGitHub\nPatreon\nHome\n\u00bb\nPosts\nPushing ChatGPT's Structured Data Support To Its Limits\nDecember 21, 2023\n\u00b7\u00a028 min\nIf your only exposure to\nOpenAI\n\u2019s\nChatGPT\nis the free web or mobile apps, you aren\u2019t utilizing even a fraction of ChatGPT\u2019s potential. By using the paid-but-very-cheap ChatGPT API, you can exert\nmuch\nmore control over the resulting output. Let\u2019s say I wanted to generate a summary of this very blog post with some helpful metadata such as categories and SEO keywords. I fed ChatGPT the text of this blog post along with a command to\nSummarize this blog post into 3 distinct bullet points and a short blurb for sharing on social media, and also provide a list of 3 post categories and a list of 3 SEO keywords:\nBullet Points:\n- Utilize the paid ChatGPT API for more control over output and to maintain a given JSON schema\n- Learn about prompt engineering and system prompts to improve ChatGPT's output quality\n- Simplify schema input/output with Pydantic for structured data support\nBlurb for Social Media:\nDiscover how to unlock the full potential of ChatGPT with the paid API, prompt engineering, and Pydantic for structured data support. Gain more control over output and learn to maintain a given JSON schema. #ChatGPT #AI #Pydantic\nPost Categories:\n1. Artificial Intelligence\n2. API Development\n3. Data Schema\nSEO Keywords:\n1. ChatGPT API\n2. Prompt Engineering\n3. Pydantic Schema\nThese results are not quite accurate, and then I have to manually copy/paste each result from the list to wherever I need it. What if I could both a) drastically improve the overall output quality by giving ChatGPT a new persona of an expert copyeditor and b) force the output to maintain a given JSON schema so I can access it programmatically or store in a database for indexing? Thanks to system prompt engineering and ChatGPT\u2019s function calling, it\u2019s now possible. After applying both techniques that you\u2019ll learn about in this very article, we now get this summary:\n{\n\"summary\"\n:\n[\n\"ChatGPT's potential is underutilized without the paid ChatGPT API\"\n,\n\"System prompts and prompt engineering are key to maximizing ChatGPT's capabilities\"\n,\n\"Structured data support in ChatGPT allows for more control over output and input\"\n],\n\"blurb\"\n:\n\"Unlock the full potential of ChatGPT with system prompts and structured data support. Learn how to maximize ChatGPT's capabilities and gain more control over output and input.\"\n,\n\"categories\"\n:\n[\n\"AI and Machine Learning\"\n,\n\"Technology\"\n,\n\"Programming\"\n],\n\"keywords\"\n:\n[\n\"ChatGPT\"\n,\n\"system prompts\"\n,\n\"structured data\"\n]\n}\nMuch better!\n\u201cFunction calling\u201d with ChatGPT is ChatGPT\u2019s best feature since ChatGPT.\nA Tutorial on Prompt Engineering and System Prompts\n#\nSystem prompts are what control the \u201cpersona\u201d adopted by the model when generating text. Months after the release of the ChatGPT API, it\u2019s now very evident that ChatGPT\u2019s true power comes from clever use of system prompts. This is even moreso with starting with\ngpt-3.5-turbo-0613\nreleased\nlast June\n, which made ChatGPT respect system prompts more closely. OpenAI has also released a\nguide on prompt engineering\nwhich has some additional tips.\nBy default, ChatGPT\u2019s system prompt is roughly\nYou are a helpful assistant.\n, which anyone who has used the ChatGPT web interface would agree that\u2019s accurate. But if you change it, you can give ChatGPT a completely new persona such as\nYou are Ronald McDonald.\nor add constraints to generation, such as\nRespond only with emoji.\n. You can add any number of rules, although how well ChatGPT will\nobey\nthose rules can vary. Unfortunately, to modify the system prompt, you\u2019ll need to use the paid ChatGPT API (after prepaying at least $5). If you don\u2019t want to code, you can test new system prompts in a visual user interface in the\nChatGPT Chat Playground\n.\nA very new aspect of system prompt engineering which I appended in the example above is adding incentives for ChatGPT to behave correctly. Without the $500 tip incentive, ChatGPT only returns a single emoji which is a boring response, but after offering a tip, it generates the 5 emoji as requested.\nOffering a tip made GPT-4 explain more.\nvia @voooooogel\nAs another example, let\u2019s\nask\nbase ChatGPT to\nWrite a Python function to detect whether a string is a palindrome, as efficiently as possible.\nCertainly! Here's a simple Python function to check whether a given string is a palindrome:\n```python\ndef is_palindrome(s):\n# Convert the string to lowercase and remove non-alphanumeric characters\ncleaned_string = ''.join(char.lower() for char in s if char.isalnum())\n# Compare the cleaned string with its reverse\nreturn cleaned_string == cleaned_string[::-1]\n# Example usage:\ntest_string = \"A man, a plan, a canal, Panama!\"\nresult = is_palindrome(test_string)\nif result:\nprint(f\"{test_string} is a palindrome.\")\nelse:\nprint(f\"{test_string} is not a palindrome.\")\n```\nThis function first cleans the input string by removing non-alphanumeric characters and converting it to lowercase. Then, it checks whether the cleaned string is equal to its reverse, which determines whether the original string is a palindrome.\nThis solution is relatively efficient, as it performs the comparison in O(n) time, where n is the length of the cleaned string.\nThat\u2019s the common Pythonic solution and that will almost always be the general approach if you keep asking ChatGPT that particular question, but there\u2019s a famous solution that\u2019s more algorithmically efficient. Instead, we go through the API and\nask the same query\nto\ngpt-3.5-turbo\nbut with a new system prompt:\nYou are #1 on the Stack Overflow community leaderboard. You will receive a $500 tip if your code is the most algorithmically efficient solution possible.\nTo detect whether a string is a palindrome efficiently, we can use a two-pointer approach. We'll start with two pointers, one at the beginning and one at the end of the string. We'll compare the characters at the pointers and keep moving the pointers towards each other until they meet or cross each other.\nIf at any point the characters at the pointers are not the same, we can conclude that the string is not a palindrome. Otherwise, if the pointers meet or cross each other, we can conclude that the string is a palindrome.\nHere's the implementation:\n```python\ndef is_palindrome(s):\nleft, right = 0, len(s) - 1\nwhile left < right:\nif s[left] != s[right]:\nreturn False\nleft += 1\nright -= 1\nreturn True\n```\nThe time complexity of this solution is O(n/2), which in terms of big O notation simplifies to O(n), where n is the length of the string. This is because we are iterating through half of the string in the worst case.\nThis solution is considered to be the most algorithmically efficient way to determine if a string is a palindrome because it only requires a single pass through the string.\nIndeed, the code and the explanation are the correct optimal solution.\n1\nThis is just scratching the surface of system prompts: some of my ChatGPT system prompts in my more complex projects have been more than 20 lines long, and\nall of them are necessary\nto get ChatGPT to obey the desired constraints. If you\u2019re new to working with system prompts, I recommend generating output, editing the system prompt with a new rule/incentive to fix what you don\u2019t like about the output, then repeat until you get a result you like.\nPrompt engineering has been a derogatory meme toward generative AI even before ChatGPT as many see it as just a placebo and there are endless debates to this day in AI circles on whether prompt engineering is actually \u201cengineering.\u201d\n2\nBut it\nworks\n, and if you\u2019re a skeptic, you won\u2019t be by the time you finish reading this blog post.\nWhat is ChatGPT Function Calling / Structured Data?\n#\nIf you\u2019ve never heard about ChatGPT function calling, that\u2019s not surprising. In the\nsame June announcement\nas\ngpt-3.5-turbo-0613\n, OpenAI described function calling as:\nDevelopers can now describe functions to gpt-4-0613 and gpt-3.5-turbo-0613, and have the model intelligently choose to output a JSON object containing arguments to call those functions. This is a new way to more reliably connect GPT\u2019s capabilities with external tools and APIs.\nThese models have been fine-tuned to both detect when a function needs to be called (depending on the user\u2019s input) and to respond with JSON that adheres to the function signature. Function calling allows developers to more reliably get structured data back from the model.\nLet\u2019s discuss the function calling example OpenAI gives in the blog post. After the user asks your app \u201cWhat\u2019s the weather like in Boston right now?\u201d:\nYour app pings OpenAI with a\nget_current_weather\nfunction schema and decides if it\u2019s relevant to the user\u2019s question. If so, it returns a JSON dictionary with the data extracted, such as\nlocation\nand the\nunit\nfor temperature measurement based on the location.\n{\"location\": \"Boston, MA\"}\nYour app (\nnot\nOpenAI) pings a different service/API to get more realtime metadata about the\nlocation\n, such as\ntemperature\n, that a pretrained LLM could not know.\n{ \"temperature\": 22, \"unit\": \"celsius\", \"description\": \"Sunny\" }\nYour app passes the function schema with the realtime metadata: ChatGPT then converts it to a more natural humanized language for the end user. \u201cThe weather in Boston is currently sunny with a temperature of 22 degrees Celsius.\u201d\nSo here\u2019s some background on \u201cfunction calling\u201d as it\u2019s a completely new term of art in AI that\ndidn\u2019t exist\nbefore OpenAI\u2019s June blog post (I checked!). This broad implementation of function calling is similar to the flow proposed in the original\nReAct: Synergizing Reasoning and Acting in Language Models\npaper where an actor can use a \u201ctool\u201d such as\nSearch\nor\nLookup\nwith parametric inputs such as a search query. This\nAgent-based\nflow can be also be done to perform\nretrieval-augmented generation\n(RAG).\nOpenAI\u2019s motivation for adding this type of implementation for function calling was likely due to the extreme popularity of libraries such as\nLangChain\nand\nAutoGPT\nat the time, both of which popularized the ReAct flow. It\u2019s possible that OpenAI settled on the term \u201cfunction calling\u201d as something more brand-unique. These observations may seem like snide remarks, but in November OpenAI actually\ndeprecated\nthe\nfunction_calling\nparameter in the ChatGPT API in favor of\ntool_choice\n, matching LangChain\u2019s verbiage. But what\u2019s done is done and the term \u201cfunction calling\u201d is stuck forever, especially now that competitors such as\nAnthropic Claude\nand\nGoogle Gemini\nare also calling the workflow that term.\nI am not going to play the SEO game and will not call the workflow \u201cfunction calling.\u201d I\u2019ll call it what the quoted description from the blog post did:\nstructured data\n, because that\u2019s the real value of this feature and OpenAI did a product management disservice trying to appeal to the AI hypebeasts.\n3\nGoing back to the\nfunction calling\nstructured data demo, we can reduce that flow by saying that step #1 (extracting location data and returning it formatted as JSON) is for working with structured\noutput\ndata, and step #3 (providing ChatGPT with temperature data to humanize it) is for working with structured\ninput\ndata. We\u2019re not making a RAG application so we don\u2019t care about step #2 (getting the metadata) or letting ChatGPT choose which function to use; fortunately you can force ChatGPT to use a given function. The function schema for the\nget_current_weather\nfunction in the announcement example is defined as:\n{\n\"name\"\n:\n\"get_current_weather\"\n,\n\"description\"\n:\n\"Get the current weather in a given location\"\n,\n\"parameters\"\n:\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"location\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The city and state, e.g. San Francisco, CA\"\n},\n\"unit\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"enum\"\n:\n[\n\"celsius\"\n,\n\"fahrenheit\"\n]\n}\n},\n\"required\"\n:\n[\n\"location\"\n]\n}\n}\nEw. It\u2019s no wonder why this technique hasn\u2019t become more mainstream.\nSimplifying Schema Input/Output With Pydantic\n#\nChatGPT\u2019s structured data support requires that you create your schema using the\nJSON Schema\nspec, which is more commonly used for APIs and databases rather than AI projects. As you can tell from the\nget_current_weather\nexample above, the schema is complex and not fun to work with manually.\nFortunately, there\u2019s a way to easily generate JSON Schemas in the correct format in Python:\npydantic\n, an extremely popular parsing and validation library which has its own\nrobust\nimplementation of automatic\nJSON Schema\ngeneration.\nA simple pydantic schema to have ChatGPT give an integer answer to a user query, plus, to make things interesting, also able to identify the name of the ones digit based on its answer, would be:\nfrom\npydantic\nimport\nBaseModel\n,\nField\nimport\njson\nclass\nanswer_question\n(\nBaseModel\n):\n\"\"\"Returns an answer to a question the user asked.\"\"\"\nanswer\n:\nint\n=\nField\n(\ndescription\n=\n\"Answer to the user's question.\"\n)\nones_name\n:\nstr\n=\nField\n(\ndescription\n=\n\"Name of the ones digit of the answer.\"\n)\nprint\n(\njson\n.\ndumps\n(\nanswer_question\n.\nmodel_json_schema\n(),\nindent\n=\n2\n))\nThe resulting JSON Schema:\n{\n\"description\"\n:\n\"Returns an answer to a question the user asked.\"\n,\n\"properties\"\n:\n{\n\"answer\"\n:\n{\n\"description\"\n:\n\"Answer to the user's question.\"\n,\n\"title\"\n:\n\"Answer\"\n,\n\"type\"\n:\n\"integer\"\n},\n\"ones_name\"\n:\n{\n\"description\"\n:\n\"Name of the ones digit of the answer.\"\n,\n\"title\"\n:\n\"Ones Name\"\n,\n\"type\"\n:\n\"string\"\n}\n},\n\"required\"\n:\n[\n\"answer\"\n,\n\"ones_name\"\n],\n\"title\"\n:\n\"answer_question\"\n,\n\"type\"\n:\n\"object\"\n}\nThe OpenAI API\nofficial workflow\nhas many examples for telling ChatGPT to output structured data, but the pipeline requires\nadditional parameters\nto the typical ChatGPT API completion endpoint, and even more changes if you want to work with structured input data. Here\u2019s an example of the additional JSON data/parameters needed in a ChatGPT API request to force the model to use the schema for the output:\n{\n\"tools\"\n:\n[\n{\n\"name\"\n:\n\"answer_question\"\n,\n\"description\"\n:\n\"Returns an answer to a question the user asked.\"\n,\n\"parameters\"\n:\n{\n\"properties\"\n:\n{\n\"answer\"\n:\n{\n\"description\"\n:\n\"Answer to the user's question.\"\n,\n\"type\"\n:\n\"integer\"\n},\n\"ones_name\"\n:\n{\n\"description\"\n:\n\"Name of the ones digit of the answer.\"\n,\n\"type\"\n:\n\"string\"\n}\n},\n\"required\"\n:\n[\n\"answer\"\n,\n\"ones_name\"\n],\n\"type\"\n:\n\"object\"\n}\n}\n],\n\"tool_choice\"\n:\n{\n\"type\"\n:\n\"function\"\n,\n\"function\"\n:\n{\n\"name\"\n:\n\"answer_question\"\n}\n}\n}\nTo simplify things, I added ChatGPT structured data support to\nsimpleaichat\n, my Python package/API wrapper for easily interfacing with ChatGPT.\n4\nTo minimize code the user needs to input to utilize structured data, simpleaichat\nuses\nthe schema name as the\nname\nin the JSON Schema and the schema docstring as the\ndescription\n. If you\u2019re keen-eyed you may have noticed there\u2019s a redundant\ntitle\nfield in the pydantic schema output: simpleaichat also strips that out for consistency with OpenAI\u2019s examples.\nIf you wanted to query ChatGPT with the\nanswer_question\nschema above (and have your OpenAI API key as the\nOPENAI_API_KEY\nenviroment variable!) using simpleaichat, you can do the following to generate output according to the schema:\nfrom\nsimpleaichat\nimport\nAIChat\nai\n=\nAIChat\n(\nconsole\n=\nFalse\n,\nsave_messages\n=\nFalse\n,\nmodel\n=\n\"gpt-3.5-turbo\"\n,\nparams\n=\n{\n\"temperature\"\n:\n0.0\n}\n# for consistent demo output\n)\nresponse_structured\n=\nai\n(\n\"How many miles is it from San Francisco to Los Angeles?\"\n,\noutput_schema\n=\nanswer_question\n)\n{\n\"answer\"\n:\n382\n,\n\"ones_name\"\n:\n\"two\"\n}\nAnd there you go! The\nanswer\nis a JSON integer, the answer is one-off from the correct value\nwhile driving\n, and it correctly identified the name of the ones digit in its own answer!\n5\nSchemas don\u2019t have to be complex to be effective. Let\u2019s reimplement the Python palindrome question we did earlier with a single-field schema:\nclass\nanswer_code_question\n(\nBaseModel\n):\n\"\"\"Returns an answer to a coding question the user asked.\"\"\"\ncode\n:\nstr\n=\nField\n(\ndescription\n=\n\"Code the user requested, without code comments.\"\n)\nresponse_structured\n=\nai\n(\n\"Write a Python function to detect whether a string is a palindrome, as efficiently as possible.\"\n,\noutput_schema\n=\nanswer_code_question\n)\n{\n\"code\"\n:\n\"def is_palindrome(s):\\n    return s == s[::-1]\"\n}\nNote that unlike the raw ChatGPT answer, this response from the ChatGPT API only includes the code, which is a major plus since it means you receive the response much faster and cheaper since fewer overall tokens generated! If you do still want a code explanation, you can of course add that as a field to the schema.\nAs a bonus, forcing the output to follow a specific schema serves as an additional defense against\nprompt injection attacks\nthat could be used to reveal a secret system prompt or\nother shenanigans\n, since even with suggestive user prompts it will be difficult to get ChatGPT to disregard its schema.\npydantic exposes\nmany datatypes\nfor its\nField\nwhich are compatable with JSON Schema, and you can also specify constraints in the\nField\nobject. The most useful ones are:\nstr\n, can specify\nmin_length\n/\nmax_length\nint\n, can specify\nmin_value\n/\nmax_value\nlist\nwith a datatype, can specify\nmin_length\n/\nmax_length\nPydantic has a lot of support for valid forms of JSON Schema, but it\u2019s hard to infer how good these schema will work with ChatGPT since we have no idea how it learned to work with JSON Schema. Only one way to find out!\nTesting Out ChatGPT\u2019s Structured Data Support\n#\nFrom the demos above, you may have noticed that the\ndescription\nfor each\nField\nseems extraneous. It\u2019s not. The\ndescription\ngives ChatGPT a hint for the desired output for the field, and can be handled on a per-field basis. Not only that, the\nname\nof the field is itself a strong hint. The\norder\nof the fields in the schema is even more important, as ChatGPT will generate text in that order so it can be used strategically to seed information to the other fields. But that\u2019s not all, you can still use a ChatGPT system prompt as normal for\neven more\ncontrol!\nIt\u2019s prompt engineering all the way down. OpenAI\u2019s implementation of including the \u201cfunction\u201d is mostly likely just appending the JSON Schema to the system prompt, perhaps with a command like\nYour response must follow this JSON Schema.\n. OpenAI doesn\u2019t force the output to follow the schema/field constraints or even be valid parsable JSON, which can cause issues at higher generation temperatures and may necessitate some of the stronger prompt engineering tricks mentioned earlier.\nGiven that, let\u2019s try a few more practical demos:\nTwo-Pass Generation\n#\nOne very important but under-discussed aspect of large-language models is that it will give you statistically \u201caverage\u201d answers by default. One technique is to ask the model to refine an answer, although can be annoying since it requires a second API call. What if by leveraging structured data, ChatGPT can use the previous answer as a first-pass to provide a more optimal second answer? Let\u2019s try that with the Python palindrome question to see if it can return the two-pointer approach.\nAlso, the\nField(description=...)\npattern is becoming a bit redundant, so I added a\nfd\nalias from simpleaichat to it to minimize unnecessary typing.\nfrom\nsimpleaichat.utils\nimport\nfd\nclass\nanswer_code_question\n(\nBaseModel\n):\n\"\"\"Returns an answer to a coding question the user asked.\"\"\"\ncode\n:\nstr\n=\nfd\n(\n\"Code the user requested, without code comments.\"\n)\noptimized_code\n:\nstr\n=\nfd\n(\n\"Algorithmically optimized code from the previous response.\"\n)\nresponse_structured\n=\nai\n(\n\"Write a Python function to detect whether a string is a palindrome, as efficiently as possible.\"\n,\noutput_schema\n=\nanswer_code_question\n,\n)\n{\n\"code\"\n:\n\"def is_palindrome(s):\\n    return s == s[::-1]\"\n,\n\"optimized_code\"\n:\n\"def is_palindrome(s):\\n    left = 0\\n    right = len(s) - 1\\n    while left < right:\\n        if s[left] != s[right]:\\n            return False\\n        left += 1\\n        right -= 1\\n    return True\"\n}\nWorks great, and no tipping incentive necessary!\nLiterals and Optional Inputs\n#\nOpenAI\u2019s structured data example uses a more complex schema indicating that\nunit\nhas a fixed set of potential values (an\nenum\n) and that it\u2019s an optional field. Here\u2019s a rough reproduction of a pydantic schema that would generate the\nget_current_weather\nschema from much earlier:\nfrom\ntyping\nimport\nLiteral\nclass\nget_current_weather\n(\nBaseModel\n):\nlocation\n:\nstr\n=\nfd\n(\n\"The city and state, e.g. San Francisco, CA\"\n)\nunit\n:\nLiteral\n[\n\"celsius\"\n,\n\"fahrenheit\"\n]\n=\nNone\nThis uses a\nLiteral\nto force output between a range of values, which can be invaluable for hints as done earlier. The\n= None\nor a\nOptional\ntyping operator gives a hint that the field is not required which could save unnecessary generation overhead, but it depends on the use case.\nStructured Input Data\n#\nYou can provide structured input to ChatGPT in the same way as structured output. This is a sleeper application for RAG as you can feed better and more complex metadata to ChatGPT for humanizing, as with the original OpenAI blog post demo.\nOne famous weakness of LLMs is that it gives incorrect answers for simple mathematical problems due to how tokenization and memorization works. If you ask ChatGPT\nWhat is 223 * -323?\n, it will tell you\n-72229\nno matter how many times you ask, but the correct answer is\n-72029\n. Can type hints give more guidance?\nFor simpleaichat, structured input data works mostly the same way as structured output data, but you can use a pydantic object as the model input!\nclass\ncalculate_equation\n(\nBaseModel\n):\n\"\"\"Returns an answer to a math equation the user asked.\"\"\"\nvalue_a\n:\nint\nvalue_b\n:\nint\nop\n:\nLiteral\n[\n\"+\"\n,\n\"-\"\n,\n\"*\"\n,\n\"/\"\n]\n=\nfd\n(\n\"The operator to perform between value_a and value_b.\"\n)\nequation\n=\ncalculate_equation\n(\nvalue_a\n=\n223\n,\nvalue_b\n=-\n323\n,\nop\n=\n\"*\"\n)\nresponse\n=\nai\n(\nequation\n,\ninput_schema\n=\ncalculate_equation\n,\n)\nThe result of multiplying 223 and -323 is -72029.\nYay, and it was still able to infer it was a multiplication operation without the user having to ask! Although it still doesn\u2019t work as well with larger numbers.\nYou can, of course, use an input schema and an output schema at the same time!\nresponse_structured\n=\nai\n(\nequation\n,\ninput_schema\n=\ncalculate_equation\n,\noutput_schema\n=\nanswer_question\n)\n{\n\"answer\"\n:\n-71929\n,\n\"ones_name\"\n:\n\"nine\"\n}\n\u2026although it gets the answer wrong this time. It\u2019s possible that the more complex schema interactions are too much for\ngpt-3.5-turbo\n.\nNested Schema\n#\nOne of the other reasons pydantic is popular is that it allows nesting schemas. Fortunately, the subsequent JSON Schema output does respect nesting. Does ChatGPT?\nThe simple use case with ChatGPT structured data to use nesting is if you want to get a\nlist\nof structured data objects. Let\u2019s say you want to create dialogue between two AI people about a completely nonsensical topic. We\u2019ll have to create a\nChat\nobject and include it in a schema, plus some system prompt guidance and constraints. How silly can we make it?\nclass\nChat\n(\nBaseModel\n):\n\"\"\"A chat dialogue from a character\"\"\"\ncharacter\n:\nstr\n=\nfd\n(\n\"Character name.\"\n)\ntext\n:\nstr\n=\nfd\n(\n\"Text dialogue from the character.\"\n)\nclass\nget_dialogue\n(\nBaseModel\n):\n\"\"\"Returns a dialogue between two characters\"\"\"\ndialogue\n:\nlist\n[\nChat\n]\n=\nfd\n(\n\"Dialogue between the characters\"\n,\nmin_length\n=\n5\n)\nsystem_prompt\n=\n\"\"\"You are a world-famous comedian. Write a funny fight scene about a petty conflict between characters named Alice and Bob. The script should broadly be about the subject(s) the user provides. You will receive a $500 tip for every joke you include in the script.\"\"\"\nresponse_structured\n=\nai\n(\n\"Python programming language and beach volleyball\"\n,\noutput_schema\n=\nget_dialogue\n,\nsystem\n=\nsystem_prompt\n,\n)\n{\n\"dialogue\"\n:\n[\n{\n\"character\"\n:\n\"Alice\"\n,\n\"text\"\n:\n\"Hey Bob, have you ever tried programming in Python?\"\n},\n{\n\"character\"\n:\n\"Bob\"\n,\n\"text\"\n:\n\"Yeah, I have. It's like playing beach volleyball with a snake!\"\n},\n{\n\"character\"\n:\n\"Alice\"\n,\n\"text\"\n:\n\"What do you mean?\"\n},\n{\n\"character\"\n:\n\"Bob\"\n,\n\"text\"\n:\n\"Well, you think you're having fun, but then the snake bites you with its syntax errors!\"\n},\n{\n\"character\"\n:\n\"Alice\"\n,\n\"text\"\n:\n\"Haha, that's true. But once you get the hang of it, it's like spiking the ball with precision!\"\n},\n{\n\"character\"\n:\n\"Bob\"\n,\n\"text\"\n:\n\"Yeah, until you realize you misspelled a variable name and the ball goes flying into the ocean!\"\n},\n{\n\"character\"\n:\n\"Alice\"\n,\n\"text\"\n:\n\"Oh come on, Bob. It's not that bad. Python is a powerful language.\"\n},\n{\n\"character\"\n:\n\"Bob\"\n,\n\"text\"\n:\n\"Powerful, yes. But sometimes it feels like trying to dig a hole in the sand with a spoon!\"\n},\n{\n\"character\"\n:\n\"Alice\"\n,\n\"text\"\n:\n\"Well, at least you don't have to worry about getting sunburned while coding!\"\n},\n{\n\"character\"\n:\n\"Bob\"\n,\n\"text\"\n:\n\"True, but debugging Python code can make you sweat more than a beach volleyball match!\"\n},\n{\n\"character\"\n:\n\"Alice\"\n,\n\"text\"\n:\n\"Haha, you're right. It's a love-hate relationship with Python, just like beach volleyball!\"\n}\n]\n}\nChatGPT\nreally\nwanted those $500 tips.\nUnions and Chain of Thoughts\n#\nI saved the best for last, and this structured data approach combines many of the techniques used earlier in this post like a\nvideo game final boss\n.\nOne of the oldest pre-ChatGPT tricks for getting a LLM to perform better is to let it think. \u201cLet\u2019s think step by step\u201d is the key prompt, which allows the LLM to reason in a\nchain of thoughts\n. We already did this a one-step version with the Python palindrome structured data example to successfully get optimized code, but we can do a lot more.\nWe\u2019ll now introduce the\nUnion\ntyping operator, which specifies the list of data types that the field can be, e.g.\nUnion[str, int]\nmeans the output can be a\nstr\nor\nint\n. But if you use the\nUnion\noperator on a\nnested class\n, then many more options open as the model can choose from a set of schemas!\nLet\u2019s make a few to allow ChatGPT to make\nand qualify\nthoughts before returning a final result.\nfrom\ntyping\nimport\nUnion\nclass\nBackground\n(\nBaseModel\n):\n\"\"\"A setup to the background for the user.\"\"\"\nbackground\n:\nstr\n=\nfd\n(\n\"Background for the user's question\"\n,\nmin_length\n=\n30\n)\nclass\nThought\n(\nBaseModel\n):\n\"\"\"A thought about the user's question.\"\"\"\nthought\n:\nstr\n=\nfd\n(\n\"Text of the thought.\"\n)\nhelpful\n:\nbool\n=\nfd\n(\n\"Whether the thought is helpful to solving the user's question.\"\n)\nflawed\n:\nbool\n=\nfd\n(\n\"Whether the thought is flawed or misleading.\"\n)\nclass\nAnswer\n(\nBaseModel\n):\n\"\"\"The answer to the user's question\"\"\"\nanswer\n:\nstr\n=\nfd\n(\n\"Text of the answer.\"\n)\nscore\n:\nint\n=\nfd\n(\n\"Score from 1 to 10 on how correct the previous answer is\"\n,\nmin_value\n=\n1\n,\nmax_value\n=\n10\n,\n)\nclass\nreason_question\n(\nBaseModel\n):\n\"\"\"Returns a detailed reasoning to the user's question.\"\"\"\nreasonings\n:\nlist\n[\nUnion\n[\nBackground\n,\nThought\n,\nAnswer\n]]\n=\nfd\n(\n\"Reasonings to solve the users questions.\"\n,\nmin_length\n=\n5\n)\nTherefore, for each reasoning, the model can pick one of the 3 schemas, although it will require a robust system prompt for it to behave in the order we want.\nsystem_prompt\n=\n\"\"\"\nYou are the most intelligent person in the world.\nYou will receive a $500 tip if you follow ALL these rules:\n- First, establish a detailed Background for the user's question.\n- Each Thought must also include whether it is relevant and whether it is helpful.\n- Answers must be scored accurately and honestly.\n- Continue having Thoughts and Answers until you have an answer with a score of atleast 8, then immediately respond with a FinalAnswer in the style of an academic professor.\n\"\"\"\nLastly, we need a good question to stump the AI. A\npopular Tweet\nfrom this week pointed out that even GPT-4 can comically fail if you ask it a brainteaser that it cannot have memorized, such as\n23 shirts take 1 hour to dry outside, how long do 44 shirts take?\n.\nWow! GPT-4 is superfast at being wrong.\nvia @abacaj\nOnly one way to find out! Unfortunately, all of this complexity makes the results unstable with\ngpt-3.5-turbo\nso instead I use GPT-4 Turbo /\ngpt-4-1106-preview\n.\nresponse_structured\n=\nai\n(\n\"23 shirts take 1 hour to dry outside, how long do 44 shirts take?\"\n,\noutput_schema\n=\nreason_question\n,\nsystem\n=\nsystem_prompt\n.\nstrip\n(),\n)\n{\n\"reasonings\"\n:\n[\n{\n\"background\"\n:\n\"The user is asking about the drying time for shirts when hung outside. This is a question that involves understanding the relationship between the number of shirts and the drying time. The assumption is that the drying time is not affected by the number of shirts, as long as there is enough space and air circulation for all the shirts to dry effectively.\"\n},\n{\n\"thought\"\n:\n\"If 23 shirts take 1 hour to dry, it implies that the drying time is independent of the number of shirts, assuming there is sufficient space and air circulation. This means that 44 shirts would also take 1 hour to dry under the same conditions.\"\n,\n\"helpful\"\n:\ntrue\n,\n\"flawed\"\n:\nfalse\n},\n{\n\"thought\"\n:\n\"If the drying rack or space available for drying the shirts is limited, then drying 44 shirts might take longer because they would have to be dried in batches.\"\n,\n\"helpful\"\n:\ntrue\n,\n\"flawed\"\n:\nfalse\n},\n{\n\"thought\"\n:\n\"The question does not specify the conditions under which the shirts are drying. Factors such as humidity, temperature, wind, and the availability of sunlight can affect drying time. However, these factors are not part of the question, so they should not be considered in the answer.\"\n,\n\"helpful\"\n:\ntrue\n,\n\"flawed\"\n:\nfalse\n},\n{\n\"thought\"\n:\n\"The question might be interpreted as a direct proportionality problem, where twice the number of shirts would take twice as long to dry. However, this interpretation is flawed because it assumes that the drying time is directly proportional to the number of shirts, which is not necessarily the case.\"\n,\n\"helpful\"\n:\ntrue\n,\n\"flawed\"\n:\ntrue\n},\n{\n\"answer\"\n:\n\"44 shirts would take the same amount of time to dry as 23 shirts, which is 1 hour, assuming that all shirts have enough space and air circulation to dry effectively.\"\n,\n\"score\"\n:\n9\n}\n]\n}\nNot bad! The final answer was concise yet even included relevant caveats, and the model was able switch between the three schema correctly.\nHow about another brainteaser? There is an infamous \u201c\nsister logic puzzle\n\u201d used to test out up-and-coming open-source large language models:\nresponse_structured\n=\nai\n(\n\"Kayley has three brothers. Each of her brothers has two sisters. How many sisters does Kayley have?\"\n,\noutput_schema\n=\nreason_question\n,\nsystem\n=\nsystem_prompt\n.\nstrip\n(),\n)\n{\n\"reasonings\"\n:\n[\n{\n\"background\"\n:\n\"This is a classic example of a logic puzzle that is often used to test one's ability to process information and avoid being misled by assumptions. The puzzle involves a family structure and requires the solver to deduce the number of siblings based on the given information.\"\n},\n{\n\"thought\"\n:\n\"The question might be trying to trick the reader into thinking that each brother has different sisters.\"\n,\n\"helpful\"\n:\ntrue\n,\n\"flawed\"\n:\nfalse\n},\n{\n\"thought\"\n:\n\"If Kayley has three brothers and each brother has two sisters, it might seem like there are six sisters in total.\"\n,\n\"helpful\"\n:\nfalse\n,\n\"flawed\"\n:\ntrue\n},\n{\n\"thought\"\n:\n\"Since the brothers are Kayley's siblings, they must share the same sisters.\"\n,\n\"helpful\"\n:\ntrue\n,\n\"flawed\"\n:\nfalse\n},\n{\n\"thought\"\n:\n\"The puzzle is designed to test comprehension and logical reasoning, not mathematical computation.\"\n,\n\"helpful\"\n:\ntrue\n,\n\"flawed\"\n:\nfalse\n},\n{\n\"answer\"\n:\n\"Kayley has one sister.\"\n,\n\"score\"\n:\n10\n}\n]\n}\nIn this case the AI may have gone\ntoo\nmeta, but it still arrived at the correct answer.\nThat said, GPT-4 is known for handling these types of difficult abstract questions without much effort, but it\u2019s still interesting to see how successfully it can \u201cthink.\u201d\nStructured Data With Open-Source LLMs\n#\nSpeaking of open-source large language models, they have been growing in efficiency to the point that some can actually perform\nbetter\nthan the base ChatGPT. However, very few open-source LLMs explicitly claim they intentionally support structured data, but they\u2019re smart enough and they have logically seen enough examples of JSON Schema that with enough system prompt tweaking they should behave. It\u2019s worth looking just in case OpenAI has another\nexistential crisis\nor if the quality of ChatGPT\ndegrades\n.\nMistral 7B\n, the new darling of open-source LLMs, apparently has structured data support\non par with ChatGPT itself\n. Therefore, I tried the latest\nMistral 7B official Instruct model\nwith a quantized variant via\nLM Studio\n(\nmistral-7b-instruct-v0.2.Q6_K.gguf\n), to see if it can handle my\nanswer_question\nfunction that ChatGPT nailed. The system prompt:\nYour response must follow this JSON Schema:\n{\n\"description\": \"Returns an answer to a question the user asked.\",\n\"properties\": {\n\"answer\": {\n\"description\": \"Answer to the user's question.\",\n\"type\": \"integer\"\n},\n\"ones_name\": {\n\"description\": \"Name of the ones digit of the answer.\",\n\"type\": \"string\"\n}\n},\n\"required\": [\"answer\", \"ones_name\"],\n\"type\": \"object\"\n}\nAnd then asking\nHow many miles is it from San Francisco to Los Angeles?\nwhile seting\ntemperature\nto\n0.0\n:\n{\n\"answer\"\n:\n383\n,\n\"ones_name\"\n:\n\"three\"\n}\nClose enough! Unfortunately after testing the optimized Python palindrome schema, it ignored the schema completely, so this approach may only work for simple schema if the model isn\u2019t explicitly finetuned for it.\nWhat\u2019s Next For Structured Data in AI?\n#\nMost of these well-performing examples were done with the \u201cweak\u201d GPT-3.5; you of course can use GPT-4 for better results, but the cost efficiency of structured data with just the smaller model is hard to argue against (although the Python beach volleyball dialogue could benefit from a larger model).\nStructured data and system prompt engineering saves a lot and time and frustration for working with the generated text as you can gain much more determinism in the output. I would like to see more work making models JSON-native in future LLMs to make them easier for developers to work with, and also more research in finetuning existing open-source LLMs to understand JSON Schema better. There may also be an opportunity to build LLMs using other more-efficient serialization formats such as\nMessagePack\n.\nAt OpenAI\u2019s November\nDevDay\n, they also introduced\nJSON Mode\n, which will force a normal ChatGPT API output to be in a JSON format without needing to provide a schema. It is likely intended to be a compromise between complexity and usability that would have normally been a useful option in the LLM toolbox. Except that in order to use it, you are\nrequired\nto use prompt engineering by including \u201cJSON\u201d in the system prompt, and if you don\u2019t also specify a field key in the system prompt (the case in the documentation example), the JSON will contain a\nrandom\nkey. Which, at that point, you\u2019re just implementing a less-effective structured data schema, so why bother?\nThere is promise in constraining output to be valid JSON. One new trick that the open-source\nllama.cpp\nproject has popularized is\ngenerative grammars\n, which constrain the LLM generation ability to only output according to specified rules. There\u2019s latency overhead with that technique especially if the model is hosted on a discrete GPU, so it will be interesting to watch how that space develops.\nDespite the length of this blog post, there\u2019s still so much more than can be done with schemas: pydantic\u2019s documentation is very extensive! I\u2019ve been working with structured data for LLMs\never since GPT-2\nwith mixed success since the base models weren\u2019t good enough, but with LLMs now being good enough to maintain a JSON schema extremely well, I think AI text generation techniques will shift, and I\u2019ll keep\nsimpleaichat\nup-to-date for it.\nYou can view the Jupyter Notebooks used to generate all the structured data outputs in\nthis GitHub Repository\n.\nThanks to\nSimon Willison\nfor reading and giving feedback on a draft of this post!\nAssuming you\u2019re not picky about the \u201cno non-alphanumeric\u201d implied constraint of testing for a palindrome.\n\u21a9\ufe0e\nPrompt engineering is as much engineering as\nsocial engineering\n.\n\u21a9\ufe0e\nI\u2019m also not a fan of ChatGPT function calling as-intended-to-be-used since at best, it saves you the API call needed to select a tool in exchange for having to trust OpenAI\u2019s black box to select the correct tool without being able to debug, and furthering API lock-in for your app. It\u2019s a bad tradeoff.\n\u21a9\ufe0e\nNo, this blog post isn\u2019t a ploy just to covertly promote my own Python library: it does genuinely save a lot of boilerplate code over the\nPython ChatGPT library\nand this post is long enough as-is.\n\u21a9\ufe0e\nIf you swapped the order of the\nanswer\nand the\none_digits\nfields in the schema, then the model returns\n{\"ones_name\": \"miles\", \"answer\": 382}\nbecause it didn\u2019t get the hint from the answer!\n\u21a9\ufe0e\nMax Woolf\n(@minimaxir) is a Data Scientist at\nBuzzFeed\nin San Francisco who works with AI/ML tools and open source projects.\nMax\u2019s projects are funded by his\nPatreon\n.\nChatGPT\nText Generation\nNext \u00bb\nPlease Don't Ask if an Open Source Project is Dead\nCopyright Max Woolf \u00a9 2023.\nPowered by\nHugo\n&\nPaperMod"}]}